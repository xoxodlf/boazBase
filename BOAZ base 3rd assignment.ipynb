{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetorized gradient descent (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 가중치가 여러개로 이루어진 식을 Gradient descent로 업데이트 해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 선형 회귀를 위한 임의의 데이터셋을 생성합니다.\n",
    "seed = 1215\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=2000, n_features=5, random_state=seed, n_informative = 5, noise = 3.0)\n",
    "\n",
    "# train_data 1600개 test_data 400개로 나누세요.\n",
    "# Your Code Here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 다음과 같은 산점도를 그려보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. h(x) = ax1+bx2+cx3+dx4+ex5라는 식을 만들어 Gradient Descent를 이용해 mse를 최소화하는 a를 찾아보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def h(theta,X):\n",
    "    return np.dot(X,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "learningrate = 0.001\n",
    "y_test = y_test.reshape(400,1)\n",
    "\n",
    "# 초기값(theta_start)은 자유입니다\n",
    "def Gradientdescent(X, y, theta_start):\n",
    "    theta = theta_start\n",
    "    \n",
    "    for meaningless in range(iterations):\n",
    "        # Your Code Here \n",
    "        # **********각 theta의 업데이트는 동시에 이루어져야 합니다 **********#\n",
    "        \n",
    "        \n",
    "        ######################################################################\n",
    "        \n",
    "    y_pred = h(theta,X_test)\n",
    "    r_square = r2_score(y_test, y_pred)    \n",
    "    \n",
    "    return print('Optimal \"a\" is:', theta, '\\n', 'R^2 :', r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "Gradientdescent(X_train,y_train, np.array([0.01,0.01,0.01,0.01,0.01]))\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 (vector로 짜기)\n",
    "iterations = 100000\n",
    "learningrate = 0.001\n",
    "y_test = y_test.reshape(400,1)\n",
    "\n",
    "# 초기값(theta_start)은 자유입니다\n",
    "def Gradientdescent(X, y, theta_start):\n",
    "    theta = theta_start\n",
    "    \n",
    "    for meaningless in range(iterations):\n",
    "        # Your Code Here \n",
    "        # ********** 각 theta의 업데이트는 동시에 이루어져야 합니다 **********\n",
    "        \n",
    "        \n",
    "        \n",
    "    y_pred = h(theta,X_test)\n",
    "    r_square = r2_score(y_test, y_pred)    \n",
    "    \n",
    "    return print('Optimal \"a\" is:', theta, '\\n', 'R^2 :', r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10만번 기준으로 약 2초정도 빠릅니다 \n",
    "# for문으로 짜면 쉽지만 속도가 느려지는 단점이 있습니다\n",
    "# 따라서, for문으로 먼저 짜고 벡터표현으로 짤 수 있는 방법을 고민해보세요 :)\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "Gradientdescent(X_train,y_train, np.array([0.01,0.01,0.01,0.01,0.01]))\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 잘 찾았는지 Linear Regression의 coefficient와 비교해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# optimal a,b based on OLS\n",
    "print(\"y = %.4fX1 + %.4fX2 + %.4fX3 + %.4fX4 + %.4fX5 + %.4f\"%\n",
    "      (float(model.coef_[0]),float(model.coef_[1]), float(model.coef_[2]), float(model.coef_[3]), \n",
    "       float(model.coef_[4]),float(model.intercept_)))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# r2_score\n",
    "r_square = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"R^2 :\",r_square)\n",
    "\n",
    "# OLS 방식으로 찾아낸 coefficient값과 매우 유사합니다\n",
    "# 우리가 찾은 모델의 R^2 값이 0.0000005 정도 높습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 test set에서 왜 오차 제곱합을 최소화하는 선형식보다 gradient descent로 찾은 선형식이 설명력이 높을까요? 그 이유를 설명해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 위 경우가 항상 가능한가요?(True/False) 를 고르고 그 이유를 설명해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear Regression with L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://i2.wp.com/laid.delanover.com/wp-content/uploads/2018/01/reg_formulas.png?w=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존의 Loss는 다음과 같습니다\n",
    "def MSE(mytheta,X,y):\n",
    "    return float((1./(len(X)*2)) * np.dot((h(mytheta,X)-y).T,(h(mytheta,X)-y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. L1 reg term을 추가한 MSE 함수를 작성해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 1-2에서 작성한 Gradientdescent 함수를 이용해 L1 reg term이 추가된 함수를 작성해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "learningrate = 0.001\n",
    "lambdaa = 0.01\n",
    "y_test = y_test.reshape(400,1)\n",
    "\n",
    "# 초기값(theta_start)은 자유입니다\n",
    "def Gradientdescent(X, y, theta_start):\n",
    "    theta = theta_start\n",
    "    \n",
    "    for meaningless in range(iterations):\n",
    "        # Your Code Here \n",
    "        # ********** 각 theta의 업데이트는 동시에 이루어져야 합니다 **********\n",
    "        # 한줄로 짜도 되고 4줄로 짜도 됩니다 뭐가 깔끔한지는 잘 모르겠네요 !\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    y_pred = h(theta,X_test)\n",
    "    r_square = r2_score(y_test, y_pred)    \n",
    "    \n",
    "    return print('Optimal \"a\" is:', theta, '\\n', 'R^2 :', r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradientdescent(X_train,y_train, np.array([0.01,0.01,0.01,0.01,0.01]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 이번에는 L2 reg term이 추가된 함수를 작성해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "learningrate = 0.001\n",
    "lambdaa = 0.01\n",
    "y_test = y_test.reshape(400,1)\n",
    "\n",
    "\n",
    "# 초기값(theta_start)은 자유입니다\n",
    "def Gradientdescent(X, y, theta_start):\n",
    "    theta = theta_start\n",
    "    \n",
    "    for meaningless in range(iterations):\n",
    "        # Your Code Here \n",
    "        # ********** 각 theta의 업데이트는 동시에 이루어져야 합니다 **********\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    y_pred = h(theta,X_test)\n",
    "    r_square = r2_score(y_test, y_pred)    \n",
    "    \n",
    "    return print('Optimal \"a\" is:', theta, '\\n', 'R^2 :', r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradientdescent(X_train,y_train, np.array([0.01,0.01,0.01,0.01,0.01]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. L1과 L2 regularization의 차이점을 gradient를 이용해 설명해보세요 왜 L1은 feature selection이 가능하고 L2는 그렇지 않은 걸까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
